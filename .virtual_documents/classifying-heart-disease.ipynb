














# Import necessary libraries
import pandas as pd
import numpy as np

# Load dataset
heart_disease_df = pd.read_csv("heart_disease.csv")

# Display dataset overview
display(heart_disease_df.head())
print("="*43)
print(" HEART DISEASE DATASET OVERVIEW ")
print("="*43, "\n")
print(heart_disease_df.info(), "\n")
print("="*43)
print(" DATA SUMMARY ")
print("="*43, "\n")
print(heart_disease_df.describe().transpose(), "\n")
print("="*43)
print(" MISSING VALUES ")
print("="*43, "\n")
print(heart_disease_df.isnull().sum(), "\n")








import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# Custom plot style
sns.set_style("dark")
plt.rcParams.update({
    "axes.facecolor": "#EBEBEB",
    "figure.facecolor": "#EBEBEB",
    "axes.edgecolor": "#EBEBEB",
    "xtick.labelsize": 14,
    "ytick.labelsize": 14,
    "axes.titlesize": 18,
    "axes.titleweight": "bold",
    "axes.labelsize": 14,
    "axes.labelpad": 14
})

# Count occurrences of heart disease cases
heart_disease_counts = heart_disease_df['present'].value_counts()

# Create a figure for the bar chart, setting size and background color
plt.figure(figsize=(12, 8))

# Create a bar plot with custom colors
ax = sns.barplot(
    x=heart_disease_counts.index.astype(str),
    y=heart_disease_counts.values,
    hue=heart_disease_counts.values,
    palette=["#B3001B", "#255C99"],
    legend=False,
    width=0.4,
    edgecolor='none'
)

# Add title and labels
ax.set_title("Distribution of Heart Disease Cases")
ax.set_xlabel("Heart Disease (1 = Yes, 0 = No)")
ax.set_ylabel("")
plt.yticks([], [])
plt.ylim(0, 200)
ax.margins(x=0.5)

# Add value annotations
for index, value in enumerate(heart_disease_counts.values):
    ax.text(index, value + 5,
            f'{value:,}',
            ha='center',
            va='bottom',
            fontsize=16,
            color="black",
            weight="bold"
           )

# Clean up borders
sns.despine(bottom=True, left=True)

# Show the plot
plt.tight_layout()
plt.show()





# List of numerical columns
numerical_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

# Create a dataframe with summary statistics
summary_df = heart_disease_df[numerical_columns].describe().T

# Add IQR and outlier boundaries
summary_df['IQR'] = summary_df['75%'] - summary_df['25%']
summary_df['Lower Bound'] = summary_df['25%'] - 1.5 * summary_df['IQR']
summary_df['Upper Bound'] = summary_df['75%'] + 1.5 * summary_df['IQR']

# Display the dataframe
summary_df


# Set up the subplots
fig, axes = plt.subplots(1, len(numerical_columns), figsize=(18, 8))

# Plot box plots for each numerical column
for i, col in enumerate(numerical_columns):
    ax = axes[i]
    sns.boxplot(data=heart_disease_df, y=col, ax=ax, color='#59C3C3',
                boxprops=dict(edgecolor="black"),
                medianprops=dict(color="black"),
                whiskerprops=dict(color="black"),
                capprops=dict(color="black"))

    ax.set_title(f'{col.capitalize()} Distribution')
    ax.set_ylabel('')
    ax.set_xlabel('')
    ax.grid(False)

# Adjust layout for better spacing
plt.tight_layout()
plt.show()








# List of categorical columns
categorical_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

# Set up the subplots (4 rows, 2 columns)
fig, axes = plt.subplots(4, 2, figsize=(18, 24))

# Flatten axes array for easy iteration
axes = axes.flatten()

# Plot bar charts for each categorical column
for i, col in enumerate(categorical_columns):
    ax = axes[i]
    sns.countplot(
        data=heart_disease_df, 
        x=col, 
        ax=ax, 
        palette="Set2", 
        hue=col, 
        legend=False, 
        edgecolor="#EBEBEB", 
        width=0.4
    )
    
    # Titles and labels
    ax.set_title(f'Distribution of {col}', pad=30)
    ax.set_xlabel('')
    ax.set_ylabel('')
    ax.set_yticks([])
    
    # Add data labels on bars
    for p in ax.patches:
        ax.annotate(
            f'{p.get_height()}', 
            (p.get_x() + p.get_width() / 2., p.get_height()), 
            ha='center', 
            va='center', 
            fontsize=12, 
            color='black', 
            xytext=(0, 5),
            textcoords='offset points'
        )

# Remove any empty subplots if categorical_columns < total axes
for j in range(len(categorical_columns), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout(pad=5)
plt.show()








# Drop the `Unnamed: 0` index column.
heart_disease_df.drop(columns="Unnamed: 0", inplace=True)
heart_disease_df.head()

# Coerce to numeric, forcing errors to NaN (if any)
heart_disease_df["ca"] = pd.to_numeric(heart_disease_df["ca"], errors="coerce")
heart_disease_df["thal"] = pd.to_numeric(heart_disease_df["thal"], errors="coerce")

# Drop rows with NaNs in 'ca' or 'thal'
heart_disease_df = heart_disease_df.dropna(subset=["ca", "thal"]).copy()

# Convert to correct types after dropping
heart_disease_df["ca"] = heart_disease_df["ca"].astype("category")
heart_disease_df["thal"] = heart_disease_df["thal"].astype("category")

# Calculate Spearman correlation matrix
corr_matrix = heart_disease_df.corr(method='spearman')

# Set up figure
plt.figure(figsize=(16, 12))

# Create heatmap
ax = sns.heatmap(
    corr_matrix,
    cmap=sns.cubehelix_palette(as_cmap=True),
    annot=True,
    linewidths=0,
    linecolor="#EBEBEB",
    fmt=".1f",
    cbar_kws={"ticks": [-1, -0.5, 0, 0.5, 1]}
)

# Customize xticks
plt.xticks(rotation=0, ha='center')
plt.yticks(rotation=0, ha='right')

# Add title
plt.title("Spearman Correlation Heatmap", pad=20)

# Display plot
plt.show()











# List of categorical columns to one-hot encode
categorical_cols = ['thal', 'ca', 'cp', 'exang', 'slope', 'sex']

# One-hot encode categorical variables, drop first to avoid dummy variable trap
heart_disease_df_encoded = pd.get_dummies(heart_disease_df, columns=categorical_cols, drop_first=True)

# Display the first few rows to confirm
heart_disease_df_encoded.head()





# Drop rows where oldpeak exceeds 4.34
heart_disease_df_encoded = heart_disease_df_encoded[heart_disease_df_encoded['oldpeak'] <= 4.34].reset_index(drop=True)

heart_disease_df_encoded['oldpeak'].value_counts(bins=10)








from sklearn.preprocessing import StandardScaler

# List of numerical columns to scale
numerical_cols = ['age', 'oldpeak', 'thalach']

# Initialize scaler
scaler = StandardScaler()

# Scale numerical columns in the encoded DataFrame
heart_disease_df_encoded[numerical_cols] = scaler.fit_transform(heart_disease_df_encoded[numerical_cols])

heart_disease_df_encoded.head()








from sklearn.model_selection import train_test_split

# Define target and features
X = heart_disease_df_encoded.drop(columns='present')
y = heart_disease_df_encoded['present']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Check class distribution in training and test sets
print("Training set class distribution:")
print(y_train.value_counts())

print("\nTest set class distribution:")
print(y_test.value_counts())





from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, confusion_matrix

# Instantiate the model
model = LogisticRegression(random_state=42, max_iter=1000)

# Fit the model on the training set
model.fit(X_train, y_train)

# Make predictions on the train set
y_pred_train = model.predict(X_train)

# Calculate train accuracy
accuracy_train = accuracy_score(y_train, y_pred_train)

# Calculate train confusion matrix
conf_matrix_train = confusion_matrix(y_train, y_pred_train)

# Sensitivity (Recall for class 1)
sensitivity_train = recall_score(y_train, y_pred_train, pos_label=1)

# Specificity (Recall for class 0)
specificity_train = recall_score(y_train, y_pred_train, pos_label=0)

# Display results
print(f"Train Accuracy: {accuracy_train:.3f}")
print(f"Train Sensitivity (Recall for class 1): {sensitivity_train:.3f}")
print(f"Train Specificity (Recall for class 0): {specificity_train:.3f}")








# Get feature names
feature_names = X_train.columns

# Get model coefficients (log-odds scale)
log_odds = model.coef_.flatten()

# Convert to odds scale
odds = np.exp(log_odds)

# Combine into a DataFrame
coef_df = pd.DataFrame({
    'Feature': feature_names,
    'Log-Odds Coefficient': log_odds,
    'Odds Ratio': odds
}).sort_values(by='Odds Ratio', ascending=False)

# Display the result
display(coef_df)








# Predict on the test set
y_pred_test = model.predict(X_test)

# Accuracy
accuracy_test = accuracy_score(y_test, y_pred_test)

# Confusion matrix
conf_matrix_test = confusion_matrix(y_test, y_pred_test)

# Sensitivity (Recall for class 1)
sensitivity_test = recall_score(y_test, y_pred_test, pos_label=1)

# Specificity (Recall for class 0)
specificity_test = recall_score(y_test, y_pred_test, pos_label=0)

# Display test evaluation metrics
print(f"Test Accuracy: {accuracy_test:.3f}")
print(f"Test Sensitivity (Recall for class 1): {sensitivity_test:.3f}")
print(f"Test Specificity (Recall for class 0): {specificity_test:.3f}")






